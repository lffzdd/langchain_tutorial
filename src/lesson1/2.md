LangChain 调用 LLM（大语言模型）其实有**三种主要方式**，每种方式适合的场景不太一样。我们先看全貌，再说哪种是实际中最常用的。

---

### 一、通过 `ChatModel` 接口（最常用 ✅）

这一种最直接，也最推荐。
用法一般是这样：

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o")
response = llm.invoke("给我讲个关于量子纠缠的笑话")
print(response)
```

这类接口适用于所有“聊天模型”（Chat 系列），包括 OpenAI、Anthropic、DeepSeek、Moonshot、Mistral 等。
LangChain 会自动把输入封装为 `HumanMessage`，输出为 `AIMessage`。
**特点：**

* 简洁清晰；
* 支持流式输出（`stream()`）；
* 可与 `Runnable` 体系（例如管道 `|` 操作符）无缝连接；
* 是目前最主流、最推荐的方式。

---

### 二、通过 `LLM` 接口（旧接口，不推荐）

老版本 LangChain 只有 `LLM` 这一类接口，例如：

```python
from langchain.llms import OpenAI

llm = OpenAI(model="text-davinci-003")
print(llm("写一首俳句"))
```

这种是“纯文本输入输出”，没有对话历史和消息角色。
**特点：**

* 适合旧式 completion 模型；
* 对新一代 chat 模型支持较弱；
* 逐渐被 `ChatModel` 接口取代。

---

### 三、通过 `Runnable` 管道或链式结构（高级方式）

LangChain 的核心机制是“可运行单元（Runnable）”，`LLM` 或 `ChatModel` 本身就是一个 `Runnable`，可以与其它组件组合，比如：

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_template("请总结以下内容：{text}")
model = ChatOpenAI(model="gpt-4o")
chain = prompt | model

result = chain.invoke({"text": "LangChain 是一个让 AI 工作流模块化的框架"})
print(result.content)
```

**特点：**

* 模块化、可扩展；
* 可插入 Memory、Retriever、Tool、OutputParser；
* 是企业级、复杂工作流常用的结构。

---

### ✅ 实际上最常用的是：

> **第二代接口 `ChatModel`（如 ChatOpenAI、ChatDeepSeek、ChatAnthropic）+ Runnable 链式结构。**

这种方式既能保持调用简洁，又能灵活地嵌入 Prompt、内存、RAG 等组件。
几乎所有现代 LangChain 应用、Agent、RAG 系统都是基于这种组合。

---

接下来你想我帮你**展示一个完整的例子**吗？比如：
一个 prompt 模板 + ChatModel + 输出解析器的标准三段式链（是现在最推荐的结构）。
